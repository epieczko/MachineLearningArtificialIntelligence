{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ARIMA-GARCH-SCA-LSTM Hybrid Model for Stock Price Forecasting\n",
    "\n",
    "This notebook implements the ARIMA-GARCH-SCA-LSTM model to forecast stock prices using log returns of stock prices. The motivation behind this choice lies in the model's ability to address both the linear and non-linear dynamics inherent in financial time series data, particularly stock market returns. \n",
    "\n",
    "## Why This Model?\n",
    "\n",
    "### 1. **ARIMA-GARCH for Trend and Volatility**\n",
    "Stock returns are known to have both linear and non-linear dependencies, and volatility clustering is a well-documented phenomenon. The ARIMA (AutoRegressive Integrated Moving Average) model is widely used to capture the linear dependencies and trends in time series data, while the GARCH (Generalized Autoregressive Conditional Heteroskedasticity) model captures time-varying volatility (volatility clustering).\n",
    "\n",
    "- **ARIMA**: It captures the trend or linear patterns in the log returns. ARIMA is appropriate for modeling stable and predictable changes over time, making it a strong choice for identifying the underlying trend.\n",
    "  \n",
    "- **GARCH**: Since stock returns tend to exhibit periods of high volatility followed by low volatility, GARCH models are effective in capturing this clustering effect, where large changes in returns are often followed by large changes, and small changes are followed by small changes. GARCH is chosen to model the time-varying volatility in stock returns.\n",
    "\n",
    "### 2. **LSTM for Non-Linear Dependencies**\n",
    "While ARIMA-GARCH is powerful for modeling linear dependencies and volatility, it is not well-suited to capture the non-linear relationships in stock price movements. For that, we use a Long Short-Term Memory (LSTM) network, a type of recurrent neural network (RNN) that excels in learning long-term dependencies in time series data.\n",
    "\n",
    "- **LSTM**: LSTMs are ideal for capturing the residual errors from ARIMA-GARCH, which represent the non-linear, chaotic elements in the stock price. By modeling the residuals, the LSTM can learn and predict the non-linear patterns that ARIMA-GARCH may fail to capture, improving overall accuracy.\n",
    "\n",
    "### 3. **Sine Cosine Algorithm (SCA) for Hyperparameter Optimization**\n",
    "The Sine Cosine Algorithm is employed to optimize the hyperparameters for both ARIMA-GARCH and LSTM. Selecting the best hyperparameters is crucial for maximizing the model's performance, but traditional grid search methods are computationally expensive. SCA provides a more efficient approach, allowing us to automatically tune key parameters such as:\n",
    "\n",
    "- For ARIMA-GARCH: ARIMA's \\(p\\), \\(d\\), \\(q\\) parameters and GARCH's volatility modeling parameters.\n",
    "- For LSTM: Hidden layer size, learning rate, and epochs.\n",
    "\n",
    "SCA alternates between exploration and exploitation, balancing the search between diverse areas of the hyperparameter space and refining near-optimal solutions. This enables the model to capture both stable trends and volatile spikes efficiently.\n",
    "\n",
    "## Summary of Model Benefits:\n",
    "\n",
    "1. **Comprehensive Modeling**: The combination of ARIMA-GARCH for stable and volatile components, and LSTM for non-linear residuals, ensures that all aspects of stock price movements are accounted for.\n",
    "  \n",
    "2. **Adaptability**: By using SCA for hyperparameter tuning, the model is adaptable to different datasets, optimizing performance without manual intervention.\n",
    "  \n",
    "3. **Financial Time Series Specificity**: The model architecture directly addresses key features of financial data, including volatility clustering and long-term dependencies, while maintaining robustness in the face of unpredictable market shocks.\n",
    "\n",
    "## Notebook Workflow:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Normalize log returns.\n",
    "   - Split the time series into training and testing sets.\n",
    "  \n",
    "2. **ARIMA-GARCH Model**:\n",
    "   - Fit the ARIMA-GARCH model to capture linear trends and volatility.\n",
    "   - Optimize ARIMA-GARCH parameters using SCA.\n",
    "\n",
    "3. **Residual Extraction**:\n",
    "   - Extract the residuals from the ARIMA-GARCH model for further analysis.\n",
    "\n",
    "4. **LSTM Model**:\n",
    "   - Train the LSTM model on the residuals from ARIMA-GARCH.\n",
    "   - Optimize LSTM parameters using SCA.\n",
    "  \n",
    "5. **Forecasting**:\n",
    "   - Combine ARIMA-GARCH and LSTM outputs for final stock price predictions.\n",
    "   - Evaluate the model's performance on a test dataset.\n",
    "\n",
    "6. **Evaluation**:\n",
    "   - Evaluate the performance using metrics such as RMSE, MAE, and assess the robustness of predictions.\n",
    "\n",
    "This model provides a strong foundation for forecasting stock price movements by leveraging advanced statistical models (ARIMA-GARCH) and deep learning techniques (LSTM) with efficient hyperparameter optimization (SCA).\n"
   ],
   "id": "f4ed040bdaab8826"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras\n",
    "\n",
    "# ARIMA and GARCH models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "\n",
    "# LSTM for deep learning\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.src.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Optimization algorithm\n",
    "import random\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import t\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "The first step in our workflow is to preprocess the stock price data. This involves:\n",
    "\n",
    "1. Fetching stock price data using the `yfinance` API.\n",
    "2. Calculating log returns for the close prices to stabilize the variance.\n",
    "3. Splitting the data into training and test sets for model development and evaluation.\n",
    "4. Scaling the log returns for input to the neural network (LSTM).\n",
    "\n",
    "We also visualize the stock price and the log returns to understand the data's underlying behavior.\n"
   ],
   "id": "3d9b888c6847ecc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_stock_data(ticker, folder='data/processed'):\n",
    "    \"\"\"\n",
    "    Load stock price data from a saved CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    ticker (str): Stock ticker symbol.\n",
    "    folder (str): Folder path where CSVs are stored.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing stock data and log returns.\n",
    "    \"\"\"\n",
    "    filename = f\"{folder}/{ticker}_processed.csv\"\n",
    "    data = pd.read_csv(filename, index_col='Date', parse_dates=True)\n",
    "\n",
    "    return data"
   ],
   "id": "d7c2f2dd77479d4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fit_arima(data, order=(1, 0, 1)):\n",
    "    \"\"\"\n",
    "    Fit ARIMA model to log returns.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing log returns.\n",
    "    order (tuple): ARIMA (p,d,q) order.\n",
    "    \n",
    "    Returns:\n",
    "    model: Fitted ARIMA model.\n",
    "    \"\"\"\n",
    "    model = ARIMA(data['Log_Return_Close_Close'], order=order)\n",
    "    arima_result = model.fit()\n",
    "    return arima_result"
   ],
   "id": "4b21bf1a3fbf3635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def fit_garch(arima_residuals, order=(1, 1)):\n",
    "    \"\"\"\n",
    "    Fit GARCH model to ARIMA residuals.\n",
    "    \n",
    "    Parameters:\n",
    "    arima_residuals (np.array): Residuals from ARIMA model.\n",
    "    order (tuple): GARCH (p, q) order.\n",
    "    \n",
    "    Returns:\n",
    "    model: Fitted GARCH model.\n",
    "    \"\"\"\n",
    "    garch_model = arch_model(arima_residuals, vol='Garch', p=order[0], q=order[1])\n",
    "    garch_result = garch_model.fit(disp='off')\n",
    "    return garch_result\n"
   ],
   "id": "b887cecadeeb237d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_lstm_data(data, look_back=5):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing log returns.\n",
    "    look_back (int): Number of time steps to consider for the LSTM input.\n",
    "    \n",
    "    Returns:\n",
    "    X_train, y_train: Training data for LSTM.\n",
    "    \"\"\"\n",
    "    # Scale data for LSTM\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    log_returns_scaled = scaler.fit_transform(data['Log_Return_Close_Close'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the dataset with look-back\n",
    "    X, y = [], []\n",
    "    for i in range(len(log_returns_scaled) - look_back):\n",
    "        X.append(log_returns_scaled[i:i+look_back])\n",
    "        y.append(log_returns_scaled[i+look_back])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    return X, y, scaler\n"
   ],
   "id": "d53a18702f28cd2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def build_lstm(input_shape, units=50):\n",
    "    \"\"\"\n",
    "    Build and compile LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of the input data for LSTM.\n",
    "    units (int): Number of LSTM units.\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled LSTM model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.LSTM(units=units, input_shape=input_shape))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ],
   "id": "1b24768d1f8368d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sine_cosine_algorithm(obj_func, dim, lb, ub, max_iter=100, n_agents=10):\n",
    "    \"\"\"\n",
    "    Sine Cosine Algorithm (SCA) for global optimization.\n",
    "    \n",
    "    Parameters:\n",
    "    obj_func (function): Objective function to minimize.\n",
    "    dim (int): Number of dimensions (parameters to optimize).\n",
    "    lb (list): Lower bounds for the parameters.\n",
    "    ub (list): Upper bounds for the parameters.\n",
    "    max_iter (int): Maximum number of iterations.\n",
    "    n_agents (int): Number of agents (candidate solutions).\n",
    "    \n",
    "    Returns:\n",
    "    list: Best solution found.\n",
    "    float: Best fitness value.\n",
    "    \"\"\"\n",
    "    # Initialize the population (random solutions within bounds)\n",
    "    positions = np.random.uniform(lb, ub, (n_agents, dim))\n",
    "\n",
    "    # Initialize best solution\n",
    "    best_pos = np.zeros(dim)\n",
    "    best_fitness = float(\"inf\")\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        for i in range(n_agents):\n",
    "            # Update position using sine and cosine\n",
    "            r1 = np.random.rand()\n",
    "            r2 = np.random.rand()\n",
    "            r3 = np.random.rand()\n",
    "            r4 = np.random.rand()\n",
    "\n",
    "            for j in range(dim):\n",
    "                if r3 < 0.5:\n",
    "                    positions[i, j] += r1 * np.sin(r2) * abs(r4 * best_pos[j] - positions[i, j])\n",
    "                else:\n",
    "                    positions[i, j] += r1 * np.cos(r2) * abs(r4 * best_pos[j] - positions[i, j])\n",
    "\n",
    "                # Enforce boundaries\n",
    "                positions[i, j] = np.clip(positions[i, j], lb[j], ub[j])\n",
    "\n",
    "        # Evaluate fitness for all agents\n",
    "        for i in range(n_agents):\n",
    "            fitness = obj_func(positions[i])\n",
    "            if fitness < best_fitness:\n",
    "                best_fitness = fitness\n",
    "                best_pos = positions[i].copy()\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}/{max_iter}, Best Fitness: {best_fitness}\")\n",
    "\n",
    "    return best_pos, best_fitness"
   ],
   "id": "4bb3611114474e94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define objective function to minimize\n",
    "def objective_function(params, data):\n",
    "    p, q, lstm_units = int(params[0]), int(params[1]), int(params[2])\n",
    "\n",
    "    # Fit ARIMA with (p, 0, q)\n",
    "    arima_result = fit_arima(data, order=(p, 0, q))\n",
    "\n",
    "    # Fit GARCH on ARIMA residuals\n",
    "    garch_result = fit_garch(arima_result.resid)\n",
    "\n",
    "    # Fit LSTM\n",
    "    X_train, y_train, scaler = prepare_lstm_data(data)\n",
    "    lstm_model = build_lstm(X_train.shape[1:], units=lstm_units)\n",
    "    lstm_model.fit(X_train, y_train, epochs=10, batch_size=1, verbose=0)\n",
    "\n",
    "    # Predict with LSTM and evaluate on training set\n",
    "    predictions = lstm_model.predict(X_train)\n",
    "    predictions_inverse = scaler.inverse_transform(predictions)\n",
    "    y_train_inverse = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "    error = mean_squared_error(y_train_inverse, predictions_inverse)\n",
    "    return error\n"
   ],
   "id": "2a02c75aeeb1dc86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def split_data(data, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Split the data into training and testing sets.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): The input dataset.\n",
    "    train_size (float): Proportion of the dataset to include in the training set (default is 0.8).\n",
    "    \n",
    "    Returns:\n",
    "    train_data (pd.DataFrame): Training set.\n",
    "    test_data (pd.DataFrame): Testing set.\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * train_size)\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "\n",
    "    return train_data, test_data\n"
   ],
   "id": "b500cfc1637b8029"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from arch import arch_model\n",
    "\n",
    "def arima_garch_model(train_data):\n",
    "    \"\"\"\n",
    "    Fit an ARIMA-GARCH model to the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data (pd.DataFrame): The input training dataset.\n",
    "    \n",
    "    Returns:\n",
    "    arima_garch_predictions (np.array): The predicted values using the ARIMA-GARCH model.\n",
    "    \"\"\"\n",
    "    # Fit ARIMA model\n",
    "    arima_model = ARIMA(train_data['Log_Return_Close_Close'], order=(5, 1, 0))\n",
    "    arima_result = arima_model.fit()\n",
    "\n",
    "    # Fit GARCH model\n",
    "    garch_model = arch_model(arima_result.resid, vol='Garch', p=1, q=1)\n",
    "    garch_result = garch_model.fit(disp=\"off\")\n",
    "\n",
    "    # Generate predictions using ARIMA + GARCH\n",
    "    forecast = arima_result.get_forecast(steps=len(train_data))\n",
    "    arima_garch_predictions = forecast.predicted_mean + garch_result.conditional_volatility\n",
    "\n",
    "    return arima_garch_predictions\n"
   ],
   "id": "f3788043821b9522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def combine_predictions(arima_garch_predictions, lstm_predictions, weight=0.5):\n",
    "    \"\"\"\n",
    "    Combine the predictions from ARIMA-GARCH and LSTM models.\n",
    "    \n",
    "    Parameters:\n",
    "    arima_garch_predictions (np.array): Predictions from the ARIMA-GARCH model.\n",
    "    lstm_predictions (np.array): Predictions from the LSTM model.\n",
    "    weight (float): The weighting to apply to the ARIMA-GARCH model (default is 0.5 for equal weighting).\n",
    "    \n",
    "    Returns:\n",
    "    combined_predictions (np.array): The combined predictions.\n",
    "    \"\"\"\n",
    "    combined_predictions = (weight * arima_garch_predictions) + ((1 - weight) * lstm_predictions)\n",
    "\n",
    "    return combined_predictions\n"
   ],
   "id": "cedd152a94fc948c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_arima_garch_sca_lstm_pipeline(ticker, data):\n",
    "    \"\"\"\n",
    "    Run the entire pipeline for the ARIMA-GARCH-SCA-LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    ticker (str): Stock ticker symbol.\n",
    "    data (pd.DataFrame): Preprocessed stock data with log returns and other features.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Final combined forecast.\n",
    "    \"\"\"\n",
    "    # Step 1: Split data into train and test sets\n",
    "    train_data, test_data = split_data(data)\n",
    "\n",
    "    # Step 2: Run ARIMA-GARCH model on the training data\n",
    "    arima_garch_predictions = arima_garch_model(train_data)\n",
    "\n",
    "    # Step 3: Run LSTM model (after SCA optimization) on the training data\n",
    "    lstm_predictions = run_sca_lstm(train_data)  # run_sca_lstm combines SCA optimization and LSTM model\n",
    "\n",
    "    # Step 4: Combine ARIMA-GARCH and LSTM predictions\n",
    "    final_predictions = combine_predictions(arima_garch_predictions, lstm_predictions)\n",
    "\n",
    "    # Step 5: Evaluate on test data (if you have evaluation code or metrics)\n",
    "    # evaluate_model(test_data, final_predictions)\n",
    "\n",
    "    return final_predictions\n"
   ],
   "id": "777798b0643fd05a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_sca_lstm(data, population_size=10, generations=50):\n",
    "    \"\"\"\n",
    "    Run Sine Cosine Algorithm (SCA) to optimize and train an LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Training data.\n",
    "    population_size (int): Number of candidates in the population for SCA.\n",
    "    generations (int): Number of generations for SCA optimization.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: Forecasted LSTM predictions.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize SCA population\n",
    "    sca_population = initialize_sca(population_size, dimension=3)  # e.g., hidden units, learning rate, etc.\n",
    "\n",
    "    # Step 2: Optimize LSTM hyperparameters using SCA\n",
    "    best_hyperparams = optimize_sca(sca_population, data, generations)\n",
    "\n",
    "    # Step 3: Train the LSTM model with the optimized hyperparameters\n",
    "    lstm_model = train_lstm(data, best_hyperparams)\n",
    "\n",
    "    # Step 4: Make predictions using the LSTM model\n",
    "    lstm_predictions = lstm_model.predict(data)\n",
    "\n",
    "    return lstm_predictions\n"
   ],
   "id": "c96d764d43fcd7c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def initialize_sca(population_size, dimension):\n",
    "    \"\"\"\n",
    "    Initialize the population for the Sine Cosine Algorithm (SCA).\n",
    "\n",
    "    Parameters:\n",
    "    population_size (int): Number of candidates in the population.\n",
    "    dimension (int): Number of hyperparameters to optimize.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Initialized population of size (population_size, dimension).\n",
    "    \"\"\"\n",
    "    # Randomly initialize population within predefined bounds\n",
    "    # For example, [hidden units, learning rate, dropout] - adjust bounds as needed\n",
    "    lower_bound = [10, 0.001, 0.1]  # min values for each parameter\n",
    "    upper_bound = [100, 0.01, 0.5]  # max values for each parameter\n",
    "\n",
    "    population = np.random.uniform(low=lower_bound, high=upper_bound, size=(population_size, dimension))\n",
    "    return population\n"
   ],
   "id": "4429858a393e4a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def update_candidate(candidate, gen, max_gen):\n",
    "    \"\"\"\n",
    "    Update the candidate solution using sine and cosine functions.\n",
    "    \n",
    "    Parameters:\n",
    "    candidate (np.array): Current candidate solution.\n",
    "    gen (int): Current generation number.\n",
    "    max_gen (int): Maximum number of generations.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Updated candidate solution.\n",
    "    \"\"\"\n",
    "    # Random numbers to control the balance between exploration and exploitation\n",
    "    r1 = np.random.uniform(0, 1)\n",
    "    r2 = np.random.uniform(0, 2 * np.pi)\n",
    "    r3 = np.random.uniform(0, 1)\n",
    "\n",
    "    # A coefficient that controls the movement direction (influence of current generation on exploration/exploitation)\n",
    "    a = r3 - (gen / max_gen)\n",
    "\n",
    "    # Update each hyperparameter in the candidate using the sine/cosine mechanism\n",
    "    updated_candidate = candidate + a * np.sin(r2) * abs(r1 * candidate) if r1 < 0.5 else candidate + a * np.cos(r2) * abs(r1 * candidate)\n",
    "\n",
    "    return updated_candidate"
   ],
   "id": "7e42c6d1e06cd2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_candidate(candidate, data):\n",
    "    \"\"\"\n",
    "    Evaluate the fitness of a candidate hyperparameter set by training an LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    candidate (np.array): Hyperparameter set to evaluate (e.g., [hidden_units, learning_rate, dropout]).\n",
    "    data (pd.DataFrame): Training data.\n",
    "    \n",
    "    Returns:\n",
    "    float: Fitness score (lower is better).\n",
    "    \"\"\"\n",
    "    # Train the LSTM model with the given candidate hyperparameters\n",
    "    model = train_lstm(data, candidate)\n",
    "\n",
    "    # Evaluate the model on validation data\n",
    "    X_val, y_val = prepare_lstm_data(data, validation=True)  # Prepare validation data\n",
    "    loss = model.evaluate(X_val, y_val)\n",
    "\n",
    "    return loss  # Lower loss means better fitness\n"
   ],
   "id": "2559f7adca57d565"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def optimize_sca(population, data, generations):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using the Sine Cosine Algorithm (SCA).\n",
    "\n",
    "    Parameters:\n",
    "    population (np.array): Initialized population of hyperparameter sets.\n",
    "    data (pd.DataFrame): Training data for the LSTM model.\n",
    "    generations (int): Number of generations to optimize over.\n",
    "\n",
    "    Returns:\n",
    "    np.array: Best hyperparameters found by SCA.\n",
    "    \"\"\"\n",
    "    best_candidate = None\n",
    "    best_fitness = float('inf')  # We're minimizing the error (fitness)\n",
    "\n",
    "    for gen in range(generations):\n",
    "        for i in range(population.shape[0]):\n",
    "            candidate = population[i]\n",
    "\n",
    "            # Update the candidate solution using the sine and cosine components (this is SCA logic)\n",
    "            candidate = update_candidate(candidate, gen, generations)\n",
    "\n",
    "            # Evaluate the candidate by training an LSTM model with those hyperparameters\n",
    "            fitness = evaluate_candidate(candidate, data)\n",
    "\n",
    "            # Track the best candidate\n",
    "            if fitness < best_fitness:\n",
    "                best_fitness = fitness\n",
    "                best_candidate = candidate\n",
    "\n",
    "    return best_candidate\n"
   ],
   "id": "ac806c2340c4ebc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_lstm(data, hyperparameters):\n",
    "    \"\"\"\n",
    "    Train an LSTM model using the optimized hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Training data for the LSTM model.\n",
    "    hyperparameters (list): Optimized hyperparameters [hidden_units, learning_rate, dropout].\n",
    "\n",
    "    Returns:\n",
    "    keras.Model: Trained LSTM model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract hyperparameters\n",
    "    hidden_units = int(hyperparameters[0])\n",
    "    learning_rate = hyperparameters[1]\n",
    "    dropout_rate = hyperparameters[2]\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(layers.LSTM(units=hidden_units, return_sequences=True, input_shape=(data.shape[1], 1)))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.LSTM(units=hidden_units))\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(1))  # Output layer for predicting stock prices\n",
    "\n",
    "    # Compile the model with Adam optimizer and mean squared error loss\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # Train the model (assuming you have prepared X_train and y_train)\n",
    "    X_train, y_train = prepare_lstm_data(data)\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32)  # Tune epochs and batch size as needed\n",
    "\n",
    "    return model\n"
   ],
   "id": "9bbeee8e78a8a73f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def run_workflow_sca_lstm(data, generations=50, population_size=10):\n",
    "    \"\"\"\n",
    "    Workflow to run Sine Cosine Algorithm (SCA) combined with LSTM model training.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): Stock log returns data.\n",
    "    generations (int): Number of generations for optimization.\n",
    "    population_size (int): Number of candidate solutions in each generation.\n",
    "    \n",
    "    Returns:\n",
    "    best_candidate: The best candidate hyperparameters found.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the population (random candidates)\n",
    "    population = initialize_sca(population_size)\n",
    "\n",
    "    best_candidate = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for gen in range(generations):\n",
    "        print(f\"Generation {gen + 1}/{generations}\")\n",
    "\n",
    "        for candidate in population:\n",
    "            # Step 2: Evaluate each candidate (train LSTM and compute fitness score)\n",
    "            fitness_score = evaluate_candidate(candidate, data)\n",
    "\n",
    "            # Step 3: Update the best solution if current candidate is better\n",
    "            if fitness_score < best_score:\n",
    "                best_score = fitness_score\n",
    "                best_candidate = candidate\n",
    "\n",
    "        # Step 4: Update population using SCA (optimize the candidates)\n",
    "        population = optimize_sca(population, gen, generations)\n",
    "\n",
    "    print(f\"Best candidate found: {best_candidate} with score {best_score}\")\n",
    "    return best_candidate\n"
   ],
   "id": "7c9fda9d5ef01162"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
